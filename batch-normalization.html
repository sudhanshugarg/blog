<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Batch Normalization | My Work & Interests</title>

  <style>
    * {
      box-sizing: border-box;
    }

    body {
      font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
      font-size: 18px;
      line-height: 1.6;
      color: rgba(0, 0, 0, 0.8);
      margin: 0;
      padding: 0;
      background: #ffffff;
    }

    .distill-header {
      background: linear-gradient(to bottom, #ffffff 0%, #f9f9f9 100%);
      border-bottom: 1px solid #e0e0e0;
      padding: 3rem 0;
      margin-bottom: 3rem;
    }

    .container {
      max-width: 768px;
      margin: 0 auto;
      padding: 0 2rem;
    }

    .back-link {
      display: inline-block;
      margin-bottom: 1rem;
      color: #4285f4;
      text-decoration: none;
      font-size: 0.95rem;
      transition: transform 0.2s;
    }

    .back-link:hover {
      transform: translateX(-3px);
    }

    h1 {
      font-size: 2.5rem;
      font-weight: 700;
      margin: 0 0 0.5rem 0;
      color: rgba(0, 0, 0, 0.9);
      line-height: 1.2;
    }

    .meta {
      font-size: 0.95rem;
      color: rgba(0, 0, 0, 0.5);
      margin: 1rem 0;
    }

    h2 {
      font-size: 1.8rem;
      font-weight: 600;
      margin: 3rem 0 1rem 0;
      color: rgba(0, 0, 0, 0.85);
      border-bottom: 2px solid #f0f0f0;
      padding-bottom: 0.5rem;
    }

    h2::before {
      content: "";
      display: block;
      width: 3rem;
      height: 3px;
      background: linear-gradient(to right, #4285f4, #ea4335);
      margin-bottom: 0.5rem;
    }

    p {
      margin: 1rem 0;
      color: rgba(0, 0, 0, 0.75);
    }

    ul {
      margin: 1rem 0;
      padding-left: 2rem;
    }

    li {
      margin: 0.5rem 0;
    }

    a {
      color: #4285f4;
      text-decoration: none;
      border-bottom: 1px solid transparent;
      transition: border-bottom 0.2s;
    }

    a:hover {
      border-bottom: 1px solid #4285f4;
    }

    .callout {
      background: #f0f9ff;
      border-left: 4px solid #4285f4;
      padding: 1rem 1.5rem;
      margin: 2rem 0;
      border-radius: 4px;
    }

    footer {
      margin-top: 5rem;
      padding: 2rem 0;
      border-top: 1px solid #e0e0e0;
      text-align: center;
      color: rgba(0, 0, 0, 0.5);
      font-size: 0.9rem;
    }
  </style>
</head>

<body>
  <header class="distill-header">
    <div class="container">
      <a href="index.html" class="back-link">‚Üê Back to Home</a>
      <h1>Batch Normalization</h1>
      <p class="meta">December 5, 2022</p>
    </div>
  </header>

  <main class="container">
    <section>
      <p>
        Paper Reading: <a href="https://arxiv.org/abs/1502.03167" target="_blank">
        Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a>
      </p>

      <div class="callout">
        <p>
          <strong>Key Insight:</strong> Batch normalization normalizes layer inputs during training,
          which stabilizes the learning process and enables faster convergence with higher learning rates.
        </p>
      </div>
    </section>

    <section>
      <h2>Some Intuition Behind Why Batch Normalization is Useful in Practice</h2>
      <ul>
        <li>Reduces training time for a deep learning network</li>
        <li>Improves gradient flow through the network (helps with vanishing gradient problem)</li>
        <li>Allows higher learning rates</li>
        <li>Reduces the strong dependence on initialization</li>
        <li>Acts as a form of regularization in a funny way, and slightly reduces the need for dropout, maybe</li>
      </ul>
    </section>

    <section>
      <h2>References</h2>
      <ul>
        <li>
          <a href="https://arxiv.org/abs/1502.03167" target="_blank">
            Paper: Batch Normalization (Ioffe & Szegedy, 2015)
          </a>
        </li>
        <li>
          <a href="https://www.youtube.com/watch?v=wEoyxE0GP2M&t=3257s" target="_blank">
            cs231n, Stanford, Lecture 6, Fei-Fei Li
          </a>
        </li>
        <li>
          <a href="https://www.youtube.com/watch?v=nUUqwaxLnWs" target="_blank">
            Why does batch norm work, Andrew Ng
          </a>
        </li>
        <li>
          <a href="https://www.youtube.com/watch?v=5qefnAek8OA" target="_blank">
            Batch Norm at Test Time, Andrew Ng
          </a>
        </li>
      </ul>
    </section>
  </main>

  <footer>
    <div class="container">
      <p>
        <a href="index.html">Back to Home</a> |
        Built with inspiration from <a href="https://distill.pub" target="_blank">Distill.pub</a>
      </p>
    </div>
  </footer>

  <script src="https://distill.pub/template.v2.js"></script>
</body>
</html>
